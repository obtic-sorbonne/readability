import os
import random
import numpy as np
from keras.utils import to_categorical
from keras.models import load_model, Sequential
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Embedding, Input, Reshape, Flatten, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate
from keras.models import Model
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import gensim
from keras import optimizers
import joblib

# Configurations
LABEL_MARK = "__label__"
DENSE_LAYER_SIZE = 200
FILTER_SIZES = [1,2, 3, 4, 5, 6, 7,8]
NB_FILTERS = 128
DROPOUT_VAL = 0.5
NUM_EPOCHS = 10
BATCH_SIZE = 32
MAX_SEQUENCE_LENGTH = 300
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2

def create_vectors(corpus_file, vectors_file):
    sentences = gensim.models.word2vec.LineSentence(corpus_file)
    model = gensim.models.Word2Vec(sentences, vector_size=EMBEDDING_DIM, window=20, min_count=0, workers=20, sg=1)

    vectors = []
    with open(vectors_file, 'w') as f:
        for word in model.wv.index_to_key:
            vector = word + " " + " ".join(str(x) for x in model.wv[word]) + "\n"
            vectors.append(vector)
            f.write(vector)

    print("Word2Vec training done.")
    return vectors

class Params:
    dense_layer_size = DENSE_LAYER_SIZE
    filter_sizes = FILTER_SIZES
    num_filters = NB_FILTERS
    dropout_val = DROPOUT_VAL
    num_epochs = NUM_EPOCHS
    batch_size = BATCH_SIZE
    inp_length = MAX_SEQUENCE_LENGTH
    embeddings_dim = EMBEDDING_DIM

class PreProcessing:
    def loadData(self, text_files):
        print("Loading data...")
        labels = []
        texts = []
        label_id = 0

        for label, text_file in text_files.items():
            with open(text_file, "r", encoding='utf-8') as f:
                for line in f:
                    labels.append(label_id)
                    texts.append(line.strip())
            label_id += 1

        tokenizer = Tokenizer()
        tokenizer.fit_on_texts(texts)
        sequences = tokenizer.texts_to_sequences(texts)

        self.word_index = tokenizer.word_index
        data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

        labels = to_categorical(np.asarray(labels))

        print('Shape of data tensor:', data.shape)
        print('Shape of label tensor:', labels.shape)

        indices = np.arange(data.shape[0])
        np.random.shuffle(indices)
        data = data[indices]
        labels = labels[indices]
        nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

        self.x_train = data[:-nb_validation_samples]
        self.y_train = labels[:-nb_validation_samples]
        self.x_val = data[-nb_validation_samples:]
        self.y_val = labels[-nb_validation_samples:]

        self.corpus_file = 'combined_corpus.txt'
        with open(self.corpus_file, 'w', encoding='utf-8') as f:
            for text in texts:
                f.write(text + "\n")

        # Sauvegarder le tokenizer
        joblib.dump(tokenizer, 'tokenizer.pkl')

    def loadEmbeddings(self, vectors_file, model_file):
        print("Creating word vectors using create_vectors...")
        vectors = create_vectors(self.corpus_file, model_file + ".vec")

        embeddings_index = {}
        for line in vectors:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs

        print('Found %s word vectors.' % len(embeddings_index))

        embedding_matrix = np.zeros((len(self.word_index) + 1, EMBEDDING_DIM))
        for word, i in self.word_index.items():
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[i] = embedding_vector

        self.embedding_matrix = embedding_matrix

        # Sauvegarder l'embedding matrix
        np.save('embedding_matrix.npy', self.embedding_matrix)

class CNNModel:
    def getModel(self, params_obj, weight=None):
        inputs = Input(shape=(params_obj.inp_length,), dtype='int32')
        embedding = Embedding(
            params_obj.vocab_size + 1,
            params_obj.embeddings_dim,
            input_length=params_obj.inp_length,
            weights=[weight],
            trainable=True
        )(inputs)

        reshape = Reshape((params_obj.inp_length, params_obj.embeddings_dim, 1))(embedding)

        conv_array = []
        maxpool_array = []
        for filter_size in params_obj.filter_sizes:
            conv = Conv2D(params_obj.num_filters, (filter_size, params_obj.embeddings_dim), activation='relu')(reshape)
            maxpool = MaxPooling2D(pool_size=(params_obj.inp_length - filter_size + 1, 1))(conv)
            conv_array.append(conv)
            maxpool_array.append(maxpool)

        deconv = Conv2DTranspose(1, (params_obj.filter_sizes[0], params_obj.embeddings_dim), activation='relu')(conv_array[0])
        deconv_model = Model(inputs=inputs, outputs=deconv)

        if len(params_obj.filter_sizes) >= 2:
            merged_tensor = Concatenate(axis=1)(maxpool_array)
            flatten = Flatten()(merged_tensor)
        else:
            flatten = Flatten()(maxpool_array[0])

        dropout = Dropout(params_obj.dropout_val)(flatten)

        hidden_dense = Dense(params_obj.dense_layer_size, kernel_initializer='uniform', activation='relu')(dropout)
        output = Dense(params_obj.num_classes, activation='softmax')(hidden_dense)

        model = Model(inputs=inputs, outputs=output)

        op = optimizers.Adam(lr=1e-3)
        model.compile(optimizer=op, loss='categorical_crossentropy', metrics=['accuracy'])

        return model, deconv_model

def train(text_files, model_file, vectors_file):
    preprocessing = PreProcessing()
    preprocessing.loadData(text_files)
    preprocessing.loadEmbeddings(vectors_file, model_file)

    params = Params()
    params.num_classes = preprocessing.y_train.shape[1]
    params.vocab_size = len(preprocessing.word_index)

    cnn_model = CNNModel()
    model, deconv_model = cnn_model.getModel(params, preprocessing.embedding_matrix)

    checkpoint = ModelCheckpoint(model_file, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
    early_stopping = EarlyStopping(monitor='val_loss', patience=3)
    callbacks_list = [checkpoint, early_stopping]

    model.fit(preprocessing.x_train, preprocessing.y_train,
              validation_data=(preprocessing.x_val, preprocessing.y_val),
              epochs=params.num_epochs, batch_size=params.batch_size,
              callbacks=callbacks_list)


# Fonction pour charger et utiliser le modèle pour la prédiction
def predict(text_file, model_file, vectors_file):
    # Charger le tokenizer et l'embedding matrix
    tokenizer = joblib.load('tokenizer.pkl')
    embedding_matrix = np.load('embedding_matrix.npy')

    # Préparer les données de prédiction
    preprocessing = PreProcessing()
    preprocessing.word_index = tokenizer.word_index

    # Charger les textes de prédiction
    with open(text_file, "r", encoding='utf-8') as f:
        texts = [line.strip() for line in f]

    sequences = tokenizer.texts_to_sequences(texts)
    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

    # Charger le modèle
    model = load_model(model_file)

    # Faire des prédictions
    predictions = model.predict(data)

    # Afficher les résultats
    class_names = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
    for i, prediction in enumerate(predictions):
        predicted_class = class_names[np.argmax(prediction)]
        print(f"Texte: {texts[i]}\nPrédiction: {predicted_class}\n")

# Exemple de texte de prédiction
text_files = {
    'A1': '/A1.txt',
    'A2': '/A2.txt',
    'B1': '/B1.txt',
    'B2': '/B2.txt',
    'C1': '/C1.txt',
    'C2': '/C2.txt'
}

# Entraîner et sauvegarder le modèle
train(text_files, 'cefr_model.h5', 'word_vectors.vec')

# Exemple d'utilisation du modèle pour la prédiction
example_prediction_file = 'example_prediction.txt'
with open(example_prediction_file, 'w', encoding='utf-8') as f:
    f.write("Tel est pris qui croyait prendre.\n")

# Charger et utiliser le modèle pour la prédiction
predict(example_prediction_file, 'cefr_model.h5', 'word_vectors.vec')
